{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST を使ったClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fashion-MNIST データセットを使って画像のclassificationを行います"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset の確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はオープンデータのfashion-MNISTを使うので、中身を確認します\n",
    "\n",
    "データの大元はここにあります。\n",
    "https://github.com/zalandoresearch/fashion-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = fashion_mnist.load_data()\n",
    "\n",
    "## <TODO> それぞれのデータ形を表示してみましょう\n",
    "## ヒント: データの型は'numpy.ndarray'です \n",
    "## https://numpy.org/doc/stable/reference/routines.array-manipulation.html でarrayの形を取得するメソッドを探してみてください。\n",
    "print(\"train feature shape\", X_train_orig.___)\n",
    "print(\"train label shape\", y_train_orig.___)\n",
    "print(\"test feature shape\", X_test_orig.___)\n",
    "print(\"test label shape\", y_test_orig.___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練データは60,000件、テストデータは10,000件で、画像のshapeは(28, 28)であることがわかります。\n",
    "また、縦横の2次元しかないため、色は白黒のグレースケール画像であることがわかります。\n",
    "\n",
    "featureとなる画像データを見てみると様々なファッションアイテムの画像が入っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def show(n_cols, n_rows, train_orig):\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(16,4))\n",
    "    for ax, pixels in zip(axs.flat, train_orig):\n",
    "        ax.imshow(pixels, cmap=\"gray\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "show(20, 5, X_train_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labelの意味は https://github.com/zalandoresearch/fashion-mnist#labels ここにあります。\n",
    "\n",
    "次に、ラベル毎の画像データの数をグラフで表示してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = [\n",
    "    'T-shirt/top',\n",
    "    'Trouser',\n",
    "    'Pullover',\n",
    "    'Dress',\n",
    "    'Coat',\n",
    "    'Sandal',\n",
    "    'Shirt',\n",
    "    'Sneaker',\n",
    "    'Bag',\n",
    "    'Ankle boot',\n",
    "]\n",
    "left = range(0, 10)\n",
    "height = np.zeros(10)\n",
    "for v in y_train_orig:\n",
    "    height[v] += 1\n",
    "    \n",
    "plt.xticks(rotation=45)\n",
    "plt.bar(left, height, tick_label=labels, align=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果から、データはそれぞれのカテゴリごとに6,000件ずつ、均等にはいっているようです。\n",
    "\n",
    "これでデータについてはわかったので、モデルを作成するための準備に取り掛かります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを作成するにあたって、データの前処理を行います。\n",
    "ここでは3つの処理を行っています。\n",
    "- kerasのcnnで使用するメソッドであるConv2Dは入力のshapeとして(batch_size, rows, cols, channels)を取るため、データをexpandします。(channelsはカラーモードに相当)\n",
    "- データの正規化を行います。 (値の範囲を[0-255]から[0-1]にします。)\n",
    "- ラベルをone hot表現に変換します。('Trouser'が正解の場合、[0,1,0,0,0,0,0,0,0,0]になります。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## <TODO> shapeを(batch_size, rows, cols)から(batch_size, rows, cols, channels)にexpandする。 (batch_sizeはtrainning時に指定するため、現時点では全データ数を指定)\n",
    "## ヒント:　https://numpy.org/doc/stable/reference/routines.array-manipulation.html でexpandするメソッドを探してみてください\n",
    "X_train = np.___(___, -1)\n",
    "X_test = np.___(___, -1)\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "\n",
    "## <TODO> グレースケールの 0-255 の値を 正規化して 0-1 の浮動小数にする\n",
    "## ヒント: ここは関数などを使わずシンプルに計算式で0-1の範囲になればokです\n",
    "X_train = ___\n",
    "X_test = ___\n",
    "\n",
    "## <TODO> one hot vectorにする\n",
    "## ヒント:　https://keras.io/ja/utils/ を参考にしてみてください\n",
    "y_train = tf.keras.utils.___(___, _)\n",
    "y_test = tf.keras.utils.___(___, _)\n",
    "\n",
    "print(\"one hot label shape\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題なく前処理が行えたら、モデルを構築していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kerasでmodelを作成する場合には2つの方法があります。 Sequential API を使う方法と、Funcional API を使う方法です。\n",
    "\n",
    "Sequential APIはシンプルで単純な構造のmodelを作る際に便利で、Functional APIは複数の入力やアウトプットをもったり、内部で分岐処理があるような複雑なモデルを作成する際に向いています。\n",
    "今回はSequential APIを用いてmodelを作ってみましょう\n",
    "\n",
    "スライドや https://www.tensorflow.org/api_docs/python/tf/keras/layers または　 https://keras.io/examples/vision/mnist_convnet/ の`Build the model`などを参考にcnnを使ってmodelを組んでみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "def cnn():\n",
    "    ## <todo> 資料を参考にモデルを組んでみましょう。\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルができたら、いよいよモデルの学習に取り掛かっていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training: 訓練\n",
    "\n",
    "では、モデルを学習させて、ついでにモデルの中身を見てみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf ./logs\n",
    "\n",
    "model = cnn()\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "## <todo> モデルをコンパイルする。\n",
    "## ヒント: https://keras.io/ja/models/model/ や資料を参考にしてみてください\n",
    "model.___(optimizer=___, loss=___, metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "## <todo> 学習を開始する。\n",
    "## バッチサイズを32,エポック数を5, 10%を検証に使うようにしてください\n",
    "## ヒント: https://keras.io/ja/models/model/ を参考にしてみてください\n",
    "model.___(x=___, y=___, ___, ___, ___, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## <todo> modelの全体像を確認する。\n",
    "## ヒント 資料を参考にしてみてください\n",
    "model.___()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練の経過や中身はどうだったでしょうか？　意図した通りのモデルが組まれており、学習も問題なく進んでいっていたでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test: 検証\n",
    "\n",
    "訓練が終わったので、訓練データには存在しないデータで検証をしていきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## <todo> modelの検証を行う。\n",
    "## ヒント　https://keras.io/ja/models/model/ を参考にしてみてください\n",
    "model.___(x=___, y=___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表示されている情報はテスト時のlossとcategorical_accuracyです。訓練の結果とどの程度違いがあったでしょうか？ overfitはしていないでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorboardでの可視化\n",
    "また、tensorboardを使うことによって訓練結果の可視化をしてみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習推移がtensorboardで表示されたでしょうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction: 推論\n",
    "\n",
    "では出来上がったmodelにリクエストを投げて実際に処理を行ってみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## <todo> modelから予測結果を受け取る。\n",
    "## ヒント https://keras.io/ja/models/model/ で予測を受け取るメソッドを探してみてください。\n",
    "predictions = model.___(x=X_test[10:20])\n",
    "\n",
    "for i, p in enumerate(predictions):\n",
    "    print(i, labels[np.argmax(p)], \"{}%\".format(p[np.argmax(p)]*100))\n",
    "show(10, 1, X_test_orig[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存\n",
    "\n",
    "出来上がったモデルは現時点では、notebookのメモリ上にしかありません。これを保存します。  \n",
    "save_formatはtensorflow2.0以降のデフォルトの保存形式になっているsaved_model形式で保存を行うための指定です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER    = \"username\" ## 自分の名前\n",
    "BUCKET  = \"mixi-ml-handson-2022\"\n",
    "VERSION = \"001\"\n",
    "\n",
    "## <todo> modelを保存する。\n",
    "## ヒント　https://keras.io/api/models/ からsaveに関する記載を探してみてください\n",
    "model.___(\"gs://{}/{}/{}\".format(BUCKET, USER, VERSION), save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 追加課題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ modelの構成を変更して、どのくらい性能が変わるか確認してみよう\n",
    "  + Dropoutの有無、正規化の有無、CNNからDNNにした場合\n",
    "+ 時間に余裕がある場合は 01ex_pruningに進もう"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
