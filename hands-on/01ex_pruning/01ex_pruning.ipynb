{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minus-beijing",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-horse",
   "metadata": {},
   "source": [
    "このチャプターでは、modelのpruningを行っていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-empire",
   "metadata": {},
   "source": [
    "### データセットの準備\n",
    "評価で使用するため、再度Fashion-MNISTデータセットをロードして、\n",
    "前処理も行なっておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = fashion_mnist.load_data()\n",
    "\n",
    "## shapeを(batch_size, rows, cols, channels)にexpandする\n",
    "X_train = np.expand_dims(X_train_orig, -1)\n",
    "X_test = np.expand_dims(X_test_orig, -1)\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "\n",
    "## グレースケールの 0-255 の値を 正規化して 0-1 の浮動小数にする\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "## one hot vectorにする\n",
    "y_train = tf.keras.utils.to_categorical(y_train_orig, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test_orig, 10)\n",
    "\n",
    "print(\"one hot label shape\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-layer",
   "metadata": {},
   "source": [
    "### モデルのロード\n",
    "01で保存したFashion-MNISTモデルをロードします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER    = \"username\" # 自分の名前\n",
    "BUCKET  = \"mixi-ml-handson-2021\"\n",
    "VERSION = \"001\"\n",
    "\n",
    "base_model = tf.keras.models.load_model(\"gs://{}/{}/{}\".format(BUCKET, USER, VERSION))\n",
    "\n",
    "# ベースモデルを一時保存しておく\n",
    "_, base_model_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(base_model, base_model_file, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-china",
   "metadata": {},
   "source": [
    "### ベースモデルの精度確認\n",
    "再度、ベースモデルの評価を確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.evaluate(X_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-pocket",
   "metadata": {},
   "source": [
    "### 重みの確認\n",
    "\n",
    "pruningとは、重みが小さいエッジを取り去って、パラメータを削減する手法になります。  \n",
    "パラメータが少なくなれば、その分モデルのサイズは小さくなり、高速化されます。  \n",
    "しかし、今回のモデルの重みに削減する余地はあるでしょうか。\n",
    "\n",
    "実際に重みの値を確認してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-disease",
   "metadata": {},
   "source": [
    "まず、再度モデルの構成を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-receipt",
   "metadata": {},
   "source": [
    "この中のうち、`conv2d`と`dense`が層を構成しています。  \n",
    "これらの層の重みからヒストグラムを作成してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_weights_histgram(model, layers_index, bins=1000):\n",
    "    ## <todo> ___を埋めて指定したindexのweightsを渡せるようにしましょう\n",
    "    weight_list = model.layers[___].weights[0].numpy().flatten()\n",
    "    plt.hist(weight_list, bins=bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "## <todo>引数 layers_indexの部分にconv2dまたはdense層のindexを入れて、それぞれの重みをplotしてみましょう\n",
    "## ヒント: モデルの構成を参考にしてみてください\n",
    "## weightsの総数が少ない場合は、binsの値を小さくしてplotしてみてください\n",
    "draw_weights_histgram(base_model, layers_index=___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-insight",
   "metadata": {},
   "source": [
    "だいたいどの層をplotしてみても、0.0付近に値が集中していたのではないでしょうか。  \n",
    "0.0付近のweightは、消去しても精度に大きな影響を与えないはずなので、このモデルにはpruningする余地が十分あるといえそうです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-entrance",
   "metadata": {},
   "source": [
    "### pruningモデルを定義\n",
    "公式の[Pruning in Keras example](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras)を参考にpruningモデルを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "def compute_necessary_steps(batch_size, epochs):\n",
    "    return np.ceil(X_train.shape[0] / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "bigin_step = 0\n",
    "end_step = compute_necessary_steps(batch_size=32, epochs=5)\n",
    "\n",
    "## <todo> Pruning in Keras exampleを参考に'pruning_shcedule'を定義してみましょう\n",
    "## 最初に10%をpruning、最終的には70%をpruningする様にスケジューリングしてみてください\n",
    "pruning_params = {\n",
    "\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(base_model, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_pruning.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-denial",
   "metadata": {},
   "source": [
    "### 学習\n",
    "pruningモデルが定義できたので、再学習させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf ./pruning_logs\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir='pruning_logs'),\n",
    "]\n",
    "model_for_pruning.fit(X_train, y_train, batch_size=32, epochs=5, validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-median",
   "metadata": {},
   "source": [
    "### 評価\n",
    "学習が終わったら、これまでと同じように評価してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_pruning.evaluate(X_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-steel",
   "metadata": {},
   "source": [
    "モデルの精度はベースモデルと比較してどうなっているでしょうか。  \n",
    "ほとんど変わってなければ、精度に影響を与えずにpruningされていることになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-dispatch",
   "metadata": {},
   "source": [
    "### 可視化\n",
    "01と同じように、学習結果をtensorboardで可視化してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir pruning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-theater",
   "metadata": {},
   "source": [
    "学習の推移やshcedule通りにpruningされていったかなどを確認してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-maple",
   "metadata": {
    "tags": []
   },
   "source": [
    "### pruningモデルを圧縮\n",
    "pruningすることが出来たので、モデルの圧縮を行いましょう。\n",
    "\n",
    "[公式](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras#create_3x_smaller_models_from_pruning)によると、圧縮を確認するには`tfmot.sparsity.keras.strip_pruning`と標準の圧縮アルゴリズムの適用（gzipなど）の両方が必要とのことなので、\n",
    "その対応をしていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "## <todo> 公式を参考に___を埋めてpruningしたmodelにstrip_pruningを適応しましょう\n",
    "model_for_export = ___\n",
    "\n",
    "# pruningしたモデルを一時保存\n",
    "_, pruned_model_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model_for_export, pruned_model_file, include_optimizer=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gzipを適応した後のsizeをkbで返す関数\n",
    "def get_gzipped_model_size_kb(file):\n",
    "    # Returns size of gzipped model, in bytes.\n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(file)\n",
    "    return int(os.path.getsize(zipped_file) / 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-assignment",
   "metadata": {},
   "source": [
    "準備ができたので、各モデルにおける圧縮の効果を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"base model size    : {} kb\".format(get_gzipped_model_size_kb(base_model_file)))\n",
    "print(\"pruned model size : {} kb\".format(get_gzipped_model_size_kb(pruned_model_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-chocolate",
   "metadata": {},
   "source": [
    "モデルが1/3ほどに圧縮されたことが確認できているでしょうか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-democrat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
